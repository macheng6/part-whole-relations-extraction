{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['protein', 'be make up of', 'amino acid'], ['vehicle', 'locate in', 'ukiah'], ['the committee', 'be make up of', 'eight member']]\n",
      "{('protein', 'amino acid'): 0, ('vehicle', 'ukiah'): 1, ('the committee', 'eight member'): 2}\n",
      "{'be make up of': 0, 'locate in': 1}\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def save(filename, data):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(data, output)\n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as output:\n",
    "        data = pickle.load(output)\n",
    "    return data\n",
    "\n",
    "path = 'E:\\Fudan\\part-whole relations\\code\\data_test.txt'\n",
    "filename1 = 'E:\\Fudan\\part-whole relations\\code\\instances_order.pkl'\n",
    "filename2 = 'E:\\Fudan\\part-whole relations\\code\\co_instance_pattern.pkl'\n",
    "\n",
    "entities_pattern = []    # 存储三元组\n",
    "with codecs.open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f.readlines():\n",
    "        tmp = ''\n",
    "        cnt = 0\n",
    "        entity = []\n",
    "        for strs in line:\n",
    "            tmp += strs\n",
    "            if strs == '\\t':\n",
    "                cnt += 1\n",
    "                if cnt <= 3:\n",
    "                    tmp = ''\n",
    "                else:\n",
    "                    entity.append(tmp.replace('\\t', ''))\n",
    "                    tmp =''\n",
    "                if cnt >= 6:\n",
    "                    entities_pattern.append(entity)\n",
    "                    break\n",
    "\n",
    "\n",
    "instances_order = {}\n",
    "patterns_order = {}\n",
    "cnt1 = 0\n",
    "cnt2 = 0\n",
    "for term in entities_pattern:\n",
    "    instance = (term[0], term[2])\n",
    "    if instance not in instances_order:\n",
    "        instances_order[instance] = cnt1\n",
    "        cnt1 += 1\n",
    "    \n",
    "    if term[1] not in patterns_order: \n",
    "        patterns_order[term[1]] = cnt2\n",
    "        cnt2 += 1\n",
    "    \n",
    "    \n",
    "\n",
    "# 共线矩阵\n",
    "co_instance_pattern = np.zeros(shape=(len(instances_order), len(patterns_order)))\n",
    "for term in entities_pattern:\n",
    "    x = instances_order[(term[0], term[2])]\n",
    "    y = patterns_order[term[1]]\n",
    "    co_instance_pattern[x][y] += 1\n",
    "    \n",
    "\n",
    "# print(entities_pattern)\n",
    "# print(instances_order)\n",
    "# print(patterns_order)\n",
    "# print(co_instance_pattern)\n",
    "save(filename1, instances_order)\n",
    "save(filename2, co_intance_pattern)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': [-0.082752, 0.67204, -0.14987, -0.064983, 0.056491, 0.40228, 0.0027747, -0.3311, -0.30691, 2.0817, 0.031819, 0.013643, 0.30265, 0.0071297, -0.5819, -0.2774, -0.062254, 1.1451, -0.24232, 0.1235, -0.12243, 0.33152, -0.006162, -0.30541, -0.13057, -0.054601, 0.037083, -0.070552, 0.5893, -0.30385, 0.2898, -0.14653, -0.27052, 0.37161, 0.32031, -0.29125, 0.0052483, -0.13212, -0.052736, 0.087349, -0.26668, -0.16897, 0.015162, -0.0083746, -0.14871, 0.23413, -0.20719, -0.091386, 0.40075, -0.17223, 0.18145, 0.37586, -0.28682, 0.37289, -0.16185, 0.18008, 0.3032, -0.13216, 0.18352, 0.095759, 0.094916, 0.008289, 0.11761, 0.34046, 0.03677, -0.29077, 0.058303, -0.027814, 0.082941, 0.1862, -0.031494, 0.27985, -0.074412, -0.13762, -0.21866, 0.18138, 0.040855, -0.113, 0.24107, 0.3657, -0.27525, -0.05684, 0.34872, 0.011884, 0.14517, -0.71395, 0.48497, 0.14807, 0.62287, 0.20599, 0.58379, -0.13438, 0.40207, 0.18311, 0.28021, -0.42349, -0.25626, 0.17715, -0.54095, 0.16596, -0.036058, 0.08499, -0.64989, 0.075549, -0.28831, 0.40626, -0.2802, 0.094062, 0.32406, 0.28437, -0.26341, 0.11553, 0.071918, -0.47215, -0.18366, -0.3]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "def save(filename, data):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(data, output)\n",
    "        \n",
    "        \n",
    "path = 'E:\\Fudan\\part-whole relations\\code\\glove.840B.300d.txt'\n",
    "filename = 'E:\\Fudan\\part-whole relations\\code\\embedding_dict.pkl'\n",
    "\n",
    "cnt = 0\n",
    "dicts = {}\n",
    "with codecs.open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f.readlines():\n",
    "        lists = line.split()\n",
    "        dicts[line[0]] = [float(x) for x in line[1:]]\n",
    "\n",
    "save(filename, dicts)\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as output:\n",
    "        data = pickle.load(output)\n",
    "    return data\n",
    "\n",
    "filename1 = 'E:\\Fudan\\part-whole relations\\cpde\\embedding_dict.pkl'\n",
    "filename2 = 'E:\\Fudan\\part-whole relations\\code\\instances_order.pkl'\n",
    "\n",
    "embedding_dict = load(filename1)\n",
    "instances_order = load(filename2)\n",
    "\n",
    "offset_list = []\n",
    "cnt = 0\n",
    "for instance in instances_order :\n",
    "        offset_list.append(abs(embedding_dict[instance[0]] - embedding_dict[instance[1]]))\n",
    "\n",
    "X = np.array(offset_list)\n",
    "kmeans = KMeans(n_cluster = k).fix(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "clusters_cents = kmeans.clusters_centers_\n",
    "\n",
    "clusters_dict = {}   # key是类别i，value是一个属于类别i的元素组成的列表\n",
    "for i in range(len(labels)):\n",
    "    if clusters_dict.get(labels[i]) == None:\n",
    "        clusters_dict[labels[i]] = [i,]\n",
    "    else:\n",
    "        clusters_dict[labels[i]].append(i)\n",
    "\n",
    "\n",
    "selected_instances = []\n",
    "order_instances = {v:k for k,v in instances_order.items()}   # 反转原来的字典\n",
    "for i in range(k):\n",
    "    minDis = float('inf')  # 初始化为最大值\n",
    "    flag = -1\n",
    "    for j in cluster_dict[i]:\n",
    "        # 计算类别i的形心与所属类别i的每个点的欧式距离\n",
    "        distance = np.sqrt(np.sum(np.square(X[j] - clusters_cents[i])))\n",
    "        if minDis > distance:\n",
    "            minDis = distance\n",
    "            flag = j\n",
    "    selected_instances.append(order_instances.[flag])\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as output:\n",
    "        data = pickle.load(output)\n",
    "    return data\n",
    "\n",
    "filename1 = 'E:\\Fudan\\part-whole relations\\cpde\\co_instance_pattern.pkl'\n",
    "filename2 = 'E:\\Fudan\\part-whole relations\\code\\instances_order.pkl'\n",
    "\n",
    "co_instance_pattern = load(filename1)\n",
    "instances_order = load(filename2)\n",
    "order_instances = {v:k for k,v in instances_order.items()}   # 反转原来的字典\n",
    "\n",
    "model = NMF(n_components=k, alpha=0.01)\n",
    "W = model.fit_transform(co_instance_pattern)\n",
    "H = model.components_\n",
    "\n",
    "idx = np.argmax(W, axis=0)   # 获得每一列的最大值所在的行索引\n",
    "\n",
    "for i in idx:\n",
    "    selected_instances.append(order_instances[i]) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
